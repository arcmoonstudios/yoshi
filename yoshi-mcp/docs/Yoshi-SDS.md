# MasterPrompt: Building the 'Yoshi-MCP' VS Code Extensions with Local LLM Inference

## Executive Summary

This comprehensive MasterPrompt provides research-backed guidance for implementing VS Code extensions with local LLM inference using the architecture: **LLM (QWEN/Llama models in GGUF format) + Rust inference engine (candle framework) + WebAssembly bridge + TypeScript shell + MCP server architecture**.

Based on extensive research across 10 technical domains, this guide delivers practical implementation strategies, performance optimization techniques, and security considerations for building production-ready VS Code extensions with local AI capabilities.

## Architecture Overview

The target architecture creates a secure, performant local AI inference system within VS Code:

```md
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Yoshi-MCP                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  TypeScript     â”‚    â”‚       MCP Server             â”‚    â”‚
â”‚  â”‚     Shell       â”‚â—„â”€â”€â–ºâ”‚    (Context Protocol)        â”‚    â”‚
â”‚  â”‚                 â”‚    â”‚                              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚  â”‚ WebAssembly â”‚â”‚    â”‚  â”‚    Candle Inference     â”‚ â”‚    â”‚
â”‚  â”‚  â”‚   Bridge    â”‚â”‚    â”‚  â”‚      Engine (Rust)      â”‚ â”‚    â”‚
â”‚  â”‚  â”‚             â”‚â”‚    â”‚  â”‚                         â”‚ â”‚    â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚â”‚    â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚    â”‚
â”‚  â”‚  â”‚  â”‚ GGUF    â”‚â”‚â”‚    â”‚  â”‚   â”‚ QWEN/Llama Models   â”‚ â”‚    â”‚
â”‚  â”‚  â”‚  â”‚ Models  â”‚â”‚â”‚    â”‚  â”‚   â”‚   (Quantized)       â”‚ â”‚    â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚    â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Phase 1: Development Environment Setup

### 1.1 Prerequisites and Toolchain Setup

**Required Tools:**

```bash
# Rust toolchain with WebAssembly target
rustup target add wasm32-unknown-unknown
rustup component add rust-src

# WebAssembly optimization tools
cargo install wasm-pack
npm install -g @wasm-tool/wasm-pack-plugin
```

**VS Code Extension Development:**

```bash
# Official VS Code extension tools
npm install -g yo generator-code
npm install -g @vscode/vsce

# TypeScript and build tools
npm install -g typescript webpack webpack-cli
```

**Model Management:**

```bash
# Hugging Face CLI for model downloading
pip install huggingface_hub
```

### 1.2 Project Structure Setup

Create the following directory structure:

```md
yoshi-mcp/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extension.ts              # Main extension entry point
â”‚   â”œâ”€â”€ mcp-server/               # MCP server implementation
â”‚   â”‚   â”œâ”€â”€ mod.rs                # MCP server main module
â”‚   â”‚   â””â”€â”€ handlers.rs           # Request handlers
â”‚   â”œâ”€â”€ inference-engine/         # Candle inference engine
â”‚   â”‚   â”œâ”€â”€ lib.rs                # Rust library entry
â”‚   â”‚   â”œâ”€â”€ model.rs              # Model loading and management
â”‚   â”‚   â””â”€â”€ inference.rs          # Inference logic
â”‚   â””â”€â”€ webassembly/              # WASM bridge
â”‚       â”œâ”€â”€ lib.rs                # WASM bindings
â”‚       â””â”€â”€ bindings.rs           # JS-WASM interface
â”œâ”€â”€ models/                       # Model storage directory
â”œâ”€â”€ package.json                  # Extension manifest
â”œâ”€â”€ Cargo.toml                    # Rust workspace configuration
â””â”€â”€ webpack.config.js             # Build configuration
```

## Phase 2: Rust Inference Engine Implementation

### 2.1 Candle Framework Setup

**Cargo.toml Configuration:**

```toml
[workspace]
members = ["src/inference-engine", "src/webassembly"]

[package]
name = "vscode-llm-inference"
version = "0.1.0"
edition = "2021"

[dependencies]
candle-core = { version = "0.4.0", features = ["cuda"] }
candle-transformers = "0.4.0"
candle-nn = "0.4.0"
candle-kernels = { version = "0.4.0", optional = true }
tokenizers = "0.19.0"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
anyhow = "1.0"
tokio = { version = "1.0", features = ["full"] }

[features]
default = ["cuda"]
cuda = ["candle-core/cuda", "candle-kernels"]
metal = ["candle-core/metal"]
```

### 2.2 Model Loading and GGUF Integration

**Model Management Implementation:**

```rust
// src/inference-engine/model.rs
use candle_core::{Device, Tensor, DType};
use candle_transformers::models::llama::{Llama, LlamaConfig};
use candle_core::quantized::gguf_file;
use std::path::Path;

pub struct ModelManager {
    device: Device,
    model: Option<Llama>,
    tokenizer: tokenizers::Tokenizer,
}

impl ModelManager {
    pub fn new() -> anyhow::Result<Self> {
        let device = if candle_core::utils::cuda_is_available() {
            Device::new_cuda(0)?
        } else {
            Device::Cpu
        };

        Ok(Self {
            device,
            model: None,
            tokenizer: Self::load_tokenizer()?,
        })
    }

    pub fn load_gguf_model(&mut self, model_path: &Path) -> anyhow::Result<()> {
        let mut file = std::fs::File::open(model_path)?;
        let content = gguf_file::Content::read(&mut file)?;

        // Extract model configuration
        let config = LlamaConfig::from_gguf(&content)?;

        // Load quantized model
        let model = Llama::from_gguf(content, &self.device)?;
        self.model = Some(model);

        Ok(())
    }

    pub async fn generate_text(
        &self,
        prompt: &str,
        max_tokens: usize,
    ) -> anyhow::Result<String> {
        let model = self.model.as_ref()
            .ok_or_else(|| anyhow::anyhow!("Model not loaded"))?;

        let tokens = self.tokenizer.encode(prompt, true)?;
        let input_ids = Tensor::new(tokens.get_ids(), &self.device)?;

        let mut generated_tokens = Vec::new();
        let mut next_token_id = input_ids;

        for _ in 0..max_tokens {
            let logits = model.forward(&next_token_id)?;
            let next_token = self.sample_token(&logits)?;

            if next_token == self.tokenizer.token_to_id("<|endoftext|>").unwrap_or(0) {
                break;
            }

            generated_tokens.push(next_token);
            next_token_id = Tensor::new(&[next_token], &self.device)?;
        }

        let output = self.tokenizer.decode(&generated_tokens, true)?;
        Ok(output)
    }

    fn sample_token(&self, logits: &Tensor) -> anyhow::Result<u32> {
        // Temperature sampling implementation
        let probabilities = candle_nn::ops::softmax(&logits, 1)?;
        // Sample from distribution (simplified)
        let token_id = probabilities.argmax(1)?.to_scalar::<u32>()?;
        Ok(token_id)
    }
}
```

### 2.3 Performance Optimization Implementation

**Optimized Inference Configuration:**

```rust
// src/inference-engine/inference.rs
use std::sync::Arc;
use tokio::sync::RwLock;

pub struct InferenceEngine {
    model_manager: Arc<RwLock<ModelManager>>,
    cache: Arc<RwLock<lru::LruCache<String, String>>>,
}

impl InferenceEngine {
    pub fn new() -> anyhow::Result<Self> {
        Ok(Self {
            model_manager: Arc::new(RwLock::new(ModelManager::new()?)),
            cache: Arc::new(RwLock::new(lru::LruCache::new(100))),
        })
    }

    pub async fn inference_with_caching(
        &self,
        prompt: &str,
        max_tokens: usize,
    ) -> anyhow::Result<String> {
        // Check cache first
        {
            let cache = self.cache.read().await;
            if let Some(cached_result) = cache.get(prompt) {
                return Ok(cached_result.clone());
            }
        }

        // Perform inference
        let model_manager = self.model_manager.read().await;
        let result = model_manager.generate_text(prompt, max_tokens).await?;

        // Cache result
        {
            let mut cache = self.cache.write().await;
            cache.put(prompt.to_string(), result.clone());
        }

        Ok(result)
    }
}
```

## Phase 3: WebAssembly Bridge Implementation

### 3.1 WASM Module Development

**WebAssembly Bindings:**

```rust
// src/webassembly/lib.rs
use wasm_bindgen::prelude::*;
use wasm_bindgen_futures::future_to_promise;
use js_sys::Promise;
use crate::inference_engine::InferenceEngine;

#[wasm_bindgen]
pub struct WasmInferenceEngine {
    engine: InferenceEngine,
}

#[wasm_bindgen]
impl WasmInferenceEngine {
    #[wasm_bindgen(constructor)]
    pub fn new() -> Result<WasmInferenceEngine, JsValue> {
        let engine = InferenceEngine::new()
            .map_err(|e| JsValue::from_str(&e.to_string()))?;

        Ok(WasmInferenceEngine { engine })
    }

    #[wasm_bindgen]
    pub fn load_model(&mut self, model_path: &str) -> Result<(), JsValue> {
        // Implementation for loading model in WASM context
        // Note: File system access limited in WASM
        Ok(())
    }

    #[wasm_bindgen]
    pub fn generate_text(
        &self,
        prompt: &str,
        max_tokens: usize,
    ) -> Promise {
        let engine = self.engine.clone();
        let prompt = prompt.to_string();

        future_to_promise(async move {
            let result = engine.inference_with_caching(&prompt, max_tokens).await
                .map_err(|e| JsValue::from_str(&e.to_string()))?;
            Ok(JsValue::from_str(&result))
        })
    }
}
```

### 3.2 WASM Build Configuration

**Build Script:**

```bash
#!/bin/bash
# build-wasm.sh

# Build with optimizations
RUSTFLAGS="-C target-feature=+simd128" \
wasm-pack build src/webassembly \
  --target web \
  --out-dir ../../dist/wasm \
  --release

# Optimize WASM binary
wasm-opt --enable-simd --enable-bulk-memory -O3 \
  dist/wasm/vscode_llm_inference_bg.wasm \
  -o dist/wasm/vscode_llm_inference_bg.wasm

# Generate TypeScript definitions
wasm-pack build src/webassembly \
  --target bundler \
  --out-dir ../../dist/wasm-ts \
  --release
```

## Phase 4: TypeScript Shell Implementation

### 4.1 Extension Entry Point

**Main Extension Implementation:**

```typescript
// src/extension.ts
import * as vscode from 'vscode';
import { WasmInferenceEngine } from '../dist/wasm-ts/vscode_llm_inference';
import { McpServer } from './mcp-server/server';

export class LocalLLMExtension {
    private wasmEngine: WasmInferenceEngine | null = null;
    private mcpServer: McpServer | null = null;

    async activate(context: vscode.ExtensionContext): Promise<void> {
        try {
            // Initialize WASM inference engine
            this.wasmEngine = new WasmInferenceEngine();

            // Initialize MCP server
            this.mcpServer = new McpServer();
            await this.mcpServer.start();

            // Register commands
            this.registerCommands(context);

            // Register providers
            this.registerProviders(context);

            vscode.window.showInformationMessage('Local LLM Extension activated');
        } catch (error) {
            vscode.window.showErrorMessage(`Failed to activate extension: ${error}`);
        }
    }

    private registerCommands(context: vscode.ExtensionContext): void {
        const generateCommand = vscode.commands.registerCommand(
            'localLLM.generateText',
            async () => {
                const editor = vscode.window.activeTextEditor;
                if (!editor) {
                    vscode.window.showWarningMessage('No active editor');
                    return;
                }

                const selection = editor.selection;
                const selectedText = editor.document.getText(selection);

                try {
                    const result = await this.wasmEngine?.generate_text(
                        selectedText,
                        100
                    );

                    if (result) {
                        await editor.edit(editBuilder => {
                            editBuilder.replace(selection, result);
                        });
                    }
                } catch (error) {
                    vscode.window.showErrorMessage(`Generation failed: ${error}`);
                }
            }
        );

        context.subscriptions.push(generateCommand);
    }

    private registerProviders(context: vscode.ExtensionContext): void {
        // Register completion provider
        const completionProvider = vscode.languages.registerCompletionItemProvider(
            { scheme: 'file' },
            new LLMCompletionProvider(this.wasmEngine),
            '.'
        );

        context.subscriptions.push(completionProvider);
    }

    deactivate(): void {
        this.mcpServer?.stop();
    }
}

export function activate(context: vscode.ExtensionContext): Promise<void> {
    const extension = new LocalLLMExtension();
    return extension.activate(context);
}

export function deactivate(): void {
    // Cleanup handled in LocalLLMExtension.deactivate()
}
```

### 4.2 Streaming and Async Patterns

**Responsive UX Implementation:**

```typescript
// src/streaming/tokenStream.ts
export class TokenStreamManager {
    private static readonly CHUNK_SIZE = 1;
    private static readonly FLUSH_INTERVAL = 50; // ms

    async streamGeneration(
        engine: WasmInferenceEngine,
        prompt: string,
        callback: (token: string) => void
    ): Promise<void> {
        return new Promise((resolve, reject) => {
            let buffer = '';
            let timeoutId: NodeJS.Timeout;

            const flushBuffer = () => {
                if (buffer.length > 0) {
                    callback(buffer);
                    buffer = '';
                }
            };

            const scheduleFlush = () => {
                clearTimeout(timeoutId);
                timeoutId = setTimeout(flushBuffer, TokenStreamManager.FLUSH_INTERVAL);
            };

            // Simulate streaming by processing generation in chunks
            engine.generate_text(prompt, 500)
                .then(result => {
                    const words = result.split(' ');
                    let index = 0;

                    const processChunk = () => {
                        if (index < words.length) {
                            buffer += words[index] + ' ';
                            index++;
                            scheduleFlush();
                            setTimeout(processChunk, 10);
                        } else {
                            flushBuffer();
                            resolve();
                        }
                    };

                    processChunk();
                })
                .catch(reject);
        });
    }
}
```

## Phase 5: MCP Server Architecture

### 5.1 MCP Server Implementation

**Core MCP Server:**

```typescript
// src/mcp-server/server.ts
import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';

export class LocalLLMMcpServer {
    private server: McpServer;
    private inferenceEngine: WasmInferenceEngine;

    constructor(inferenceEngine: WasmInferenceEngine) {
        this.inferenceEngine = inferenceEngine;
        this.server = new McpServer({
            name: 'local-llm-server',
            version: '1.0.0'
        }, {
            capabilities: {
                tools: {},
                resources: {},
                prompts: {}
            }
        });

        this.setupHandlers();
    }

    private setupHandlers(): void {
        // Register tool handlers
        this.server.setRequestHandler('tools/list', async () => ({
            tools: [
                {
                    name: 'generate_code',
                    description: 'Generate code based on natural language description',
                    inputSchema: {
                        type: 'object',
                        properties: {
                            description: { type: 'string' },
                            language: { type: 'string' },
                            max_tokens: { type: 'number', default: 200 }
                        },
                        required: ['description']
                    }
                }
            ]
        }));

        this.server.setRequestHandler('tools/call', async (request) => {
            const { name, arguments: args } = request.params;

            if (name === 'generate_code') {
                const result = await this.inferenceEngine.generate_text(
                    args.description,
                    args.max_tokens || 200
                );

                return {
                    content: [
                        {
                            type: 'text',
                            text: result
                        }
                    ]
                };
            }

            throw new Error(`Unknown tool: ${name}`);
        });

        // Register resource handlers
        this.server.setRequestHandler('resources/list', async () => ({
            resources: [
                {
                    uri: 'local://workspace/files',
                    name: 'Workspace Files',
                    description: 'Access to workspace file contents'
                }
            ]
        }));
    }

    async start(): Promise<void> {
        const transport = new StdioServerTransport();
        await this.server.connect(transport);
    }

    stop(): void {
        this.server.close();
    }
}
```

### 5.2 Context Management

**Workspace Context Provider:**

```typescript
// src/mcp-server/contextProvider.ts
export class WorkspaceContextProvider {
    private workspaceRoot: string;

    constructor(workspaceRoot: string) {
        this.workspaceRoot = workspaceRoot;
    }

    async getRelevantFiles(query: string): Promise<string[]> {
        const files = await this.getAllFiles();
        return files.filter(file =>
            this.isFileRelevant(file, query)
        );
    }

    private async getAllFiles(): Promise<string[]> {
        const files: string[] = [];

        async function* walk(dir: string): AsyncGenerator<string> {
            const dirents = await fs.promises.readdir(dir, { withFileTypes: true });

            for (const dirent of dirents) {
                const path = `${dir}/${dirent.name}`;

                if (dirent.isDirectory()) {
                    yield* walk(path);
                } else if (this.isCodeFile(dirent.name)) {
                    yield path;
                }
            }
        }

        for await (const file of walk(this.workspaceRoot)) {
            files.push(file);
        }

        return files;
    }

    private isCodeFile(filename: string): boolean {
        const codeExtensions = ['.js', '.ts', '.py', '.rs', '.go', '.java', '.cpp'];
        return codeExtensions.some(ext => filename.endsWith(ext));
    }

    private isFileRelevant(file: string, query: string): boolean {
        // Simple relevance scoring based on filename and query
        const filename = file.toLowerCase();
        const queryLower = query.toLowerCase();

        return filename.includes(queryLower) ||
               queryLower.split(' ').some(word => filename.includes(word));
    }
}
```

## Phase 6: Security and Error Handling

### 6.1 Yoshi Framework Integration

**Structured Error Handling:**

```rust
// src/inference-engine/error.rs
use yoshi::prelude::*;

#[derive(Debug, Diagnostic, thiserror::Error)]
pub enum InferenceError {
    #[error("Model loading failed: {source}")]
    #[diagnostic(code(inference::model_load))]
    ModelLoadError {
        #[source]
        source: Box<dyn std::error::Error + Send + Sync>,
        #[label("Model file path")]
        path: String,
    },

    #[error("Inference failed: {message}")]
    #[diagnostic(
        code(inference::generation),
        help("Try reducing max_tokens or checking input format")
    )]
    GenerationError {
        message: String,
        #[label("Input prompt")]
        prompt: String,
    },

    #[error("WASM bridge error: {details}")]
    #[diagnostic(code(inference::wasm))]
    WasmError {
        details: String,
    },
}

impl From<anyhow::Error> for InferenceError {
    fn from(err: anyhow::Error) -> Self {
        InferenceError::GenerationError {
            message: err.to_string(),
            prompt: "Unknown".to_string(),
        }
    }
}
```

### 6.2 Security Implementation

**Sandboxed Model Execution:**

```rust
// src/inference-engine/security.rs
use std::process::{Command, Stdio};
use tokio::process::Command as AsyncCommand;

pub struct SecureInferenceRunner {
    max_memory_mb: usize,
    timeout_seconds: u64,
}

impl SecureInferenceRunner {
    pub fn new(max_memory_mb: usize, timeout_seconds: u64) -> Self {
        Self {
            max_memory_mb,
            timeout_seconds,
        }
    }

    pub async fn run_inference(
        &self,
        model_path: &str,
        prompt: &str,
    ) -> Result<String, InferenceError> {
        // Create isolated process for inference
        let mut cmd = AsyncCommand::new("timeout")
            .arg(self.timeout_seconds.to_string())
            .arg("systemd-run")
            .arg("--user")
            .arg("--scope")
            .arg(format!("--property=MemoryMax={}", self.max_memory_mb * 1024 * 1024))
            .arg("--")
            .arg("./inference-runner")
            .arg(model_path)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .spawn()
            .map_err(|e| InferenceError::GenerationError {
                message: format!("Failed to spawn secure process: {}", e),
                prompt: prompt.to_string(),
            })?;

        // Send prompt to process
        if let Some(stdin) = cmd.stdin.as_mut() {
            use tokio::io::AsyncWriteExt;
            stdin.write_all(prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }

        // Wait for completion with timeout
        let output = cmd.wait_with_output().await
            .map_err(|e| InferenceError::GenerationError {
                message: format!("Process execution failed: {}", e),
                prompt: prompt.to_string(),
            })?;

        if output.status.success() {
            Ok(String::from_utf8_lossy(&output.stdout).to_string())
        } else {
            Err(InferenceError::GenerationError {
                message: String::from_utf8_lossy(&output.stderr).to_string(),
                prompt: prompt.to_string(),
            })
        }
    }
}
```

## Phase 7: Model Distribution and Management

### 7.1 Secure Model Distribution

**Model Manager with Verification:**

```typescript
// src/models/modelManager.ts
import * as crypto from 'crypto';
import * as fs from 'fs';
import * as path from 'path';

export class ModelManager {
    private modelDir: string;
    private verificationKeys: Map<string, string>;

    constructor(modelDir: string) {
        this.modelDir = modelDir;
        this.verificationKeys = new Map();
        this.loadVerificationKeys();
    }

    async downloadModel(
        modelId: string,
        expectedHash: string
    ): Promise<string> {
        const modelPath = path.join(this.modelDir, `${modelId}.gguf`);

        // Download model (simplified)
        await this.downloadFile(
            `https://huggingface.co/${modelId}/resolve/main/model.gguf`,
            modelPath
        );

        // Verify integrity
        const actualHash = await this.calculateFileHash(modelPath);
        if (actualHash !== expectedHash) {
            fs.unlinkSync(modelPath);
            throw new Error(`Model integrity check failed: ${modelId}`);
        }

        return modelPath;
    }

    private async calculateFileHash(filePath: string): Promise<string> {
        return new Promise((resolve, reject) => {
            const hash = crypto.createHash('sha256');
            const stream = fs.createReadStream(filePath);

            stream.on('data', data => hash.update(data));
            stream.on('end', () => resolve(hash.digest('hex')));
            stream.on('error', reject);
        });
    }

    private async downloadFile(url: string, destination: string): Promise<void> {
        // Implementation for secure file download with progress
        // Include retry logic and cancellation support
    }

    private loadVerificationKeys(): void {
        // Load trusted model hashes and verification keys
        const keysPath = path.join(this.modelDir, 'verification-keys.json');
        if (fs.existsSync(keysPath)) {
            const keys = JSON.parse(fs.readFileSync(keysPath, 'utf8'));
            this.verificationKeys = new Map(Object.entries(keys));
        }
    }
}
```

### 7.2 Configuration Management

**Extension Configuration:**

```json
{
  "name": "local-llm-assistant",
  "displayName": "Local LLM Assistant",
  "version": "1.0.0",
  "engines": {
    "vscode": "^1.85.0"
  },
  "categories": ["AI", "Programming Languages"],
  "activationEvents": [
    "onStartupFinished"
  ],
  "main": "./dist/extension.js",
  "contributes": {
    "commands": [
      {
        "command": "localLLM.generateText",
        "title": "Generate Text with Local LLM"
      },
      {
        "command": "localLLM.selectModel",
        "title": "Select Local Model"
      }
    ],
    "configuration": {
      "title": "Local LLM",
      "properties": {
        "localLLM.modelPath": {
          "type": "string",
          "default": "",
          "description": "Path to local GGUF model file"
        },
        "localLLM.maxTokens": {
          "type": "number",
          "default": 200,
          "description": "Maximum tokens to generate"
        },
        "localLLM.temperature": {
          "type": "number",
          "default": 0.7,
          "minimum": 0.0,
          "maximum": 2.0,
          "description": "Sampling temperature"
        },
        "localLLM.enableGPU": {
          "type": "boolean",
          "default": true,
          "description": "Enable GPU acceleration if available"
        }
      }
    }
  }
}
```

## Phase 8: Performance Optimization and Monitoring

### 8.1 Performance Monitoring

**Telemetry and Metrics:**

```typescript
// src/telemetry/performanceMonitor.ts
export class PerformanceMonitor {
    private metrics: Map<string, number[]> = new Map();

    startTiming(operation: string): string {
        const id = `${operation}-${Date.now()}-${Math.random()}`;
        this.metrics.set(id, [performance.now()]);
        return id;
    }

    endTiming(id: string): number {
        const times = this.metrics.get(id);
        if (!times) return 0;

        const duration = performance.now() - times[0];
        times.push(duration);

        return duration;
    }

    getAverageTime(operation: string): number {
        const allTimes = Array.from(this.metrics.entries())
            .filter(([key]) => key.startsWith(operation))
            .flatMap(([, times]) => times.slice(1));

        return allTimes.length > 0
            ? allTimes.reduce((a, b) => a + b) / allTimes.length
            : 0;
    }

    reportMetrics(): void {
        const report = {
            inference_avg_ms: this.getAverageTime('inference'),
            model_load_avg_ms: this.getAverageTime('model_load'),
            total_inferences: this.getTotalInferences(),
            timestamp: new Date().toISOString()
        };

        // Send to telemetry service (if enabled by user)
        console.log('Performance Report:', report);
    }

    private getTotalInferences(): number {
        return Array.from(this.metrics.keys())
            .filter(key => key.startsWith('inference'))
            .length;
    }
}
```

### 8.2 Optimization Strategies

**Build Optimization:**

```javascript
// webpack.config.js
const path = require('path');

module.exports = {
    target: 'node',
    entry: './src/extension.ts',
    output: {
        path: path.resolve(__dirname, 'dist'),
        filename: 'extension.js',
        libraryTarget: 'commonjs2'
    },
    module: {
        rules: [
            {
                test: /\.ts$/,
                use: 'ts-loader',
                exclude: /node_modules/
            },
            {
                test: /\.wasm$/,
                type: 'asset/resource'
            }
        ]
    },
    resolve: {
        extensions: ['.ts', '.js', '.wasm']
    },
    externals: {
        vscode: 'commonjs vscode'
    },
    optimization: {
        minimize: true,
        splitChunks: {
            chunks: 'all',
            cacheGroups: {
                wasm: {
                    test: /\.wasm$/,
                    name: 'wasm-modules',
                    chunks: 'all'
                }
            }
        }
    }
};
```

## Phase 9: Testing and Validation

### 9.1 Unit Testing Framework

**Test Suite Setup:**

```typescript
// src/test/suite/extension.test.ts
import * as assert from 'assert';
import * as vscode from 'vscode';
import { LocalLLMExtension } from '../../extension';

suite('Extension Test Suite', () => {
    vscode.window.showInformationMessage('Start all tests.');

    test('Extension activation', async () => {
        const extension = new LocalLLMExtension();
        const context = {
            subscriptions: [],
            extensionPath: __dirname,
            globalState: {
                get: () => undefined,
                update: () => Promise.resolve()
            }
        } as any;

        await extension.activate(context);
        assert.ok(true, 'Extension activated successfully');
    });

    test('Model loading', async () => {
        // Test model loading functionality
        const modelPath = path.join(__dirname, 'fixtures', 'test-model.gguf');
        // Implementation of model loading test
    });

    test('Text generation', async () => {
        // Test text generation functionality
        const prompt = "Hello, world!";
        // Implementation of generation test
    });
});
```

### 9.2 Integration Testing

**End-to-End Testing:**

```typescript
// src/test/integration/e2e.test.ts
import * as vscode from 'vscode';
import * as path from 'path';

suite('E2E Integration Tests', () => {
    let extension: vscode.Extension<any>;

    setup(async () => {
        extension = vscode.extensions.getExtension('your-publisher.local-llm-assistant')!;
        await extension.activate();
    });

    test('Complete workflow', async () => {
        // 1. Open a document
        const document = await vscode.workspace.openTextDocument({
            content: 'function hello() {\n  // Generate code here\n}',
            language: 'javascript'
        });

        const editor = await vscode.window.showTextDocument(document);

        // 2. Select text
        editor.selection = new vscode.Selection(1, 2, 1, 25);

        // 3. Execute command
        await vscode.commands.executeCommand('localLLM.generateText');

        // 4. Verify result
        const updatedContent = editor.document.getText();
        assert.ok(updatedContent.includes('console.log'), 'Generated code should be present');
    });
});
```

## Phase 10: Deployment and Distribution

### 10.1 Extension Packaging

**Build and Package Script:**

```json
{
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "webpack --mode production",
    "compile:dev": "webpack --mode development --watch",
    "build:wasm": "./scripts/build-wasm.sh",
    "test": "node ./out/test/runTest.js",
    "package": "vsce package",
    "publish": "vsce publish"
  }
}
```

### 10.2 CI/CD Pipeline

**GitHub Actions Workflow:**

```yaml
name: Build and Test

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'

    - name: Setup Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        target: wasm32-unknown-unknown

    - name: Install dependencies
      run: |
        npm install
        cargo install wasm-pack

    - name: Build WASM
      run: npm run build:wasm

    - name: Build extension
      run: npm run compile

    - name: Run tests
      run: npm test

    - name: Package extension
      run: npm run package

    - name: Upload VSIX
      uses: actions/upload-artifact@v3
      with:
        name: extension-vsix
        path: '*.vsix'
```

## Current Yoshi Framework Architecture

```mmd
flowchart LR
        %% ArcMoon Studios FlowMap for Project: yoshi
        %% Calculated Code Quality: Good Quality (Composite Score: 0.929)
        %% Research Sources: 8 validated sources integrated
        %% Analysis Mode: M1 - Complete Workspace Analysis
        %% Processing Iterations: 1
        Start([ğŸš€ Rust Workspace Analysis<br/>Project Structure]) --> Workspace[Workspace Root<br/>ğŸ“Š Crate Dependencies]

    subgraph Foundation["ğŸ—ï¸ Foundation Layer"]
        yoshi_core
        subgraph yoshi_core_API["ğŸ“‹ yoshi-core API"]
            yoshi_core_capture_timestamp([ğŸ”§ capture_timestamp<br/>1 params])
            click yoshi_core_capture_timestamp "https://docs.rs/yoshi_core/latest/yoshi_core/fn.capture_timestamp.html" "fn capture_timestamp(self: &Self) -> SystemTime | Returns the capture timestamp." _blank
            yoshi_core_downcast_mut([ğŸ”§ downcast_mut<br/>1 params])
            click yoshi_core_downcast_mut "https://docs.rs/yoshi_core/latest/yoshi_core/fn.downcast_mut.html" "fn downcast_mut(self: &mut Self) -> Option | Returns a mutable reference to the underlying foreign error (if `YoshiKind::Foreign`)." _blank
            yoshi_core_add_location([ğŸ”§ add_location<br/>2 params])
            click yoshi_core_add_location "https://docs.rs/yoshi_core/latest/yoshi_core/fn.add_location.html" "fn add_location(self: &mut Self, location: YoshiLocation) | Adds a location to the backtrace chain." _blank
            yoshi_core_with_location([ğŸ”§ with_location<br/>2 params])
            click yoshi_core_with_location "https://docs.rs/yoshi_core/latest/yoshi_core/fn.with_location.html" "fn with_location(self: Self, location: YoshiLocation) -> Self | Sets location information on the error's primary nest." _blank
            yoshi_core_new_captured([ğŸ”§ new_captured<br/>no params])
            click yoshi_core_new_captured "https://docs.rs/yoshi_core/latest/yoshi_core/fn.new_captured.html" "fn new_captured() -> Self | Creates a new minimal backtrace for `no_std` environments." _blank
        end
        yoshi_core -.->|API| yoshi_core_API
        class yoshi_core_capture_timestamp fnNode
        class yoshi_core_downcast_mut fnNode
        class yoshi_core_add_location fnNode
        class yoshi_core_with_location fnNode
        class yoshi_core_new_captured fnNode
        yoshi_std
        subgraph yoshi_std_API["ğŸ“‹ yoshi-std API"]
            yoshi_std_with_timeout([âš¡ with_timeout<br/>3 params])
            click yoshi_std_with_timeout "https://docs.rs/yoshi_std/latest/yoshi_std/fn.with_timeout.html" "async fn with_timeout(duration: Duration, future: F, operation_name: &str) -> Result | Timeout wrapper that converts timeout errors to Yoshi errors

# Examples

```rust,no_run
use yoshi_s..." _blank
            yoshi_std_clear([ğŸ”§ clear<br/>1 params])
            click yoshi_std_clear "https://docs.rs/yoshi_std/latest/yoshi_std/fn.clear.html" "fn clear(self: &mut Self) | Clears the buffer contents while preserving the allocated capacity." _blank
            yoshi_std_thread_id([ğŸ”§ thread_id<br/>1 params])
            click yoshi_std_thread_id "https://docs.rs/yoshi_std/latest/yoshi_std/fn.thread_id.html" "fn thread_id(self: &Self) -> ThreadId | Returns the thread ID where this backtrace was captured." _blank
            yoshi_std_new([ğŸ”§ new<br/>2 params])
            click yoshi_std_new "https://docs.rs/yoshi_std/latest/yoshi_std/fn.new.html" "fn new(line: u32, character: u32) -> Self | Creates a new position." _blank
            yoshi_std_capture_std_backtrace([ğŸ”§ capture_std_backtrace<br/>no params])
            click yoshi_std_capture_std_backtrace "https://docs.rs/yoshi_std/latest/yoshi_std/fn.capture_std_backtrace.html" "fn capture_std_backtrace() -> Option | Conditionally captures a `StdYoshiBacktrace` based on environment variables." _blank
        end
        yoshi_std -.->|API| yoshi_std_API
        class yoshi_std_with_timeout asyncFn
        class yoshi_std_clear fnNode
        class yoshi_std_thread_id fnNode
        class yoshi_std_new fnNode
        class yoshi_std_capture_std_backtrace fnNode
    end

    subgraph Advanced["âš¡ Advanced Layer"]
        yoshi_deluxe
        yoshi_derive
    end

    Workspace --> yoshi[yoshi<br/>Library Crate<br/>Complexity: Medium]
    click yoshi "https://docs.rs/yoshi/latest/yoshi/" "19 files Â· 23 deps Â· 18 features Â· library Â· tests | Click to view documentation" _blank
    yoshi -.->|docs| yoshi_anchor["ğŸ“š yoshi docs<br/>Local anchor: #yoshi"]
    class yoshi_anchor docLink
    yoshi --> yoshi_features[Features: default, std, derive...]
    yoshi --> yoshi_lib[lib.rs]
    yoshi --> yoshi_main[main.rs]
    Workspace --> yoshi_analyzer[yoshi-analyzer<br/>Library Crate<br/>Complexity: Medium]
    click yoshi_analyzer "https://docs.rs/yoshi-analyzer/latest/yoshi_analyzer/" "12 files Â· 29 deps Â· no features Â· library Â· no tests | Click to view documentation" _blank
    yoshi_analyzer -.->|docs| yoshi_analyzer_anchor["ğŸ“š yoshi-analyzer docs<br/>Local anchor: #yoshi_analyzer"]
    class yoshi_analyzer_anchor docLink
    yoshi_analyzer --> yoshi_analyzer_lib[lib.rs]
    yoshi_analyzer --> yoshi_analyzer_main[main.rs]
    yoshi_analyzer --> yoshi_analyzer_analyzers[analyzers.rs]
    Workspace --> yoshi_benches[yoshi-benches<br/>Library Crate<br/>Complexity: Low]
    click yoshi_benches "https://docs.rs/yoshi-benches/latest/yoshi_benches/" "15 files Â· 14 deps Â· 3 features Â· library Â· tests | Click to view documentation" _blank
    yoshi_benches -.->|docs| yoshi_benches_anchor["ğŸ“š yoshi-benches docs<br/>Local anchor: #yoshi_benches"]
    class yoshi_benches_anchor docLink
    yoshi_benches --> yoshi_benches_features[Features: default, comparison, std]
    yoshi_benches --> yoshi_benches_lib[lib.rs]
    yoshi_benches --> yoshi_benches_comprehensive_comparison[comprehensive_comparison.rs]
    Workspace --> yoshi_core[yoshi-core<br/>Core Foundation<br/>Complexity: Low]
    click yoshi_core "https://docs.rs/yoshi-core/latest/yoshi_core/" "5 files Â· 3 deps Â· 6 features Â· library Â· tests | Click to view documentation" _blank
    yoshi_core -.->|docs| yoshi_core_anchor["ğŸ“š yoshi-core docs<br/>Local anchor: #yoshi_core"]
    class yoshi_core_anchor docLink
    yoshi_core --> yoshi_core_features[Features: default, alloc, std...]
    yoshi_core --> yoshi_core_lib[lib.rs]
    Workspace --> yoshi_deluxe[yoshi-deluxe<br/>Advanced Features<br/>Complexity: High]
    click yoshi_deluxe "https://docs.rs/yoshi-deluxe/latest/yoshi_deluxe/" "25 files Â· 41 deps Â· 12 features Â· library Â· tests | Click to view documentation" _blank
    yoshi_deluxe -.->|docs| yoshi_deluxe_anchor["ğŸ“š yoshi-deluxe docs<br/>Local anchor: #yoshi_deluxe"]
    class yoshi_deluxe_anchor docLink
    yoshi_deluxe --> yoshi_deluxe_features[Features: default, runtime-analysis, config-validation...]
    yoshi_deluxe --> yoshi_deluxe_lib[lib.rs]
    yoshi_deluxe --> yoshi_deluxe_ast[ast.rs]
    yoshi_deluxe --> yoshi_deluxe_codegen[codegen.rs]
    Workspace --> yoshi_derive[yoshi-derive<br/>Proc Macro Crate<br/>Complexity: Low]
    click yoshi_derive "https://docs.rs/yoshi-derive/latest/yoshi_derive/" "15 files Â· 13 deps Â· 7 features Â· proc-macro Â· tests | Click to view documentation" _blank
    yoshi_derive -.->|docs| yoshi_derive_anchor["ğŸ“š yoshi-derive docs<br/>Local anchor: #yoshi_derive"]
    class yoshi_derive_anchor docLink
    yoshi_derive --> yoshi_derive_features[Features: default, std, optimize-large...]
    yoshi_derive --> yoshi_derive_lib[lib.rs]
    Workspace --> yoshi_std[yoshi-std<br/>Std Integration<br/>Complexity: Low]
    click yoshi_std "https://docs.rs/yoshi-std/latest/yoshi_std/" "4 files Â· 6 deps Â· 11 features Â· library Â· tests | Click to view documentation" _blank
    yoshi_std -.->|docs| yoshi_std_anchor["ğŸ“š yoshi-std docs<br/>Local anchor: #yoshi_std"]
    class yoshi_std_anchor docLink
    yoshi_std --> yoshi_std_features[Features: default, derive, serde...]
    yoshi_std --> yoshi_std_lib[lib.rs]

    %% Inter-crate Dependencies
    yoshi -->|Foundation Dependency| yoshi_core
    yoshi -->|Standard Integration| yoshi_std
    yoshi -.->|Macro Generation| yoshi_derive
    yoshi --> yoshi_deluxe
    yoshi_analyzer -->|Foundation Dependency| yoshi_core
    yoshi_analyzer -->|Standard Integration| yoshi_std
    yoshi_analyzer -.->|Macro Generation| yoshi_derive
    yoshi_analyzer --> yoshi_deluxe
    yoshi_benches --> yoshi
    yoshi_benches -->|Standard Integration| yoshi_std
    yoshi_deluxe -->|Standard Integration| yoshi_std
    yoshi_deluxe -->|Foundation Dependency| yoshi_core
    yoshi_deluxe -.->|Macro Generation| yoshi_derive
    yoshi_derive -->|Foundation Dependency| yoshi_core
    yoshi_derive -->|Standard Integration| yoshi_std
    yoshi_std -->|Foundation Dependency| yoshi_core

    %% Analysis Results
    Workspace -.->|Issue Analysis| IssueAnalysis[Static Analysis Results<br/>Comprehensive Resolution<br/>Realistic 2-5&#37; allocation improvement &#40;only &#126;30&#37; of Vec::new&#40;&#41; calls optimizable&#41;]
    Workspace -.->|Issue Analysis| IssueAnalysis[Static Analysis Results<br/>Comprehensive Resolution<br/>Realistic 2-5&#37; allocation improvement &#40;only &#126;30&#37; of Vec::new&#40;&#41; calls optimizable&#41;]
    Workspace -.->|ğŸ”— Interface Analysis| InterfaceAnalysis[API Design Evaluation<br/>Pattern: Best practices<br/>Ergonomics: Research-validated<br/>RAP Enhancement: Diagram clarity improvements possible]

    %% Research Augmentation Protocol (RAP) Findings - Interactive Links
    Workspace -.->|Research| RAP0[Memory Allocation Optimization<br/>Realistic 2-5&#37; allocation improvement &#40;...]
    click RAP0 "https://doc.rust-lang.org/std/vec/struct.Vec.html#method.with_capacity" "Vec::with_capacity and arena allocation patterns. Realistic 2-5&#37; allocation improvement &#40;only &#126;30&#37; o..." _blank
    Workspace -.->|Research| RAP1[Compilation Time Optimization<br/>Build time optimization opportunities]
    click RAP1 "https://doc.rust-lang.org/cargo/reference/features.html" "Incremental compilation with feature flags. Build time optimization opportunities" _blank
    Workspace -.->|Research| RAP2[Cognitive Load Reduction Techniques<br/>Diagram clarity improvements possible]
    click RAP2 "https://mermaid-js.github.io/mermaid/" "Research-backed visual hierarchy and color theory. Diagram clarity improvements possible" _blank

    %% Quality Assessment
    Workspace -.->|provisional| QualityCert[ğŸ“Š Quality Assessment: Good<br/>Score: 0.929]

    %% ArcMoon Studios Enhanced Styling Classes with Cognitive Load Optimization
    classDef primeAnnotation fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#01579b
    classDef qualityCert fill:#fff8e1,stroke:#ff8f00,stroke-width:3px,color:#e65100
    classDef fnNode fill:#ffffff,stroke:#1976d2,stroke-width:1px,color:#000000
    classDef unsafeFn fill:#ffebee,stroke:#c62828,stroke-width:2px,color:#b71c1c
    classDef asyncFn fill:#e8f5e8,stroke:#388e3c,stroke-width:1px,color:#1b5e20
    classDef fallibleFn fill:#fff3e0,stroke:#f57c00,stroke-width:1px,color:#e65100
    classDef coreNode fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#1b5e20
    classDef stdNode fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1
    classDef deluxeNode fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#4a148c
    classDef procMacroNode fill:#ffebee,stroke:#c62828,stroke-width:2px,color:#b71c1c
    classDef libraryNode fill:#f5f5f5,stroke:#616161,stroke-width:2px,color:#424242

    %% Apply ArcMoon Studios FlowMap Generator styling to crate nodes
    class yoshi libraryNode
    class yoshi_analyzer libraryNode
    class yoshi_benches libraryNode
    class yoshi_core coreNode
    class yoshi_deluxe deluxeNode
    class yoshi_derive procMacroNode
    class yoshi_std stdNode

    %% Apply styling to annotation nodes
    class IssueAnalysis,IssueAnalysis,InterfaceAnalysis primeAnnotation
    class QualityCert qualityCert

    %% Safety Color Legend (emojis only in legend)
    LegendUnsafe([unsafe]):::unsafeFn
    LegendAsync([async]):::asyncFn
    LegendFallible([fallible]):::fallibleFn
    LegendRegular([regular]):::fnNode

    %% Connect legend to prevent orphan nodes
    Start -.-> LegendRegular

    %% ArcMoon Studios Research Integration Footer
    %% Generated by FlowMap Generator - Quality Assessment Complete
    %% Quality Score: 0.929 | Level: Good
    %% Processing Iterations: 1 | Analysis Depth: Elite Level
```

## Implementation Guidelines and Best Practices

### Security Considerations

1. **Sandboxing**: Run inference in isolated processes with resource limits
2. **Input Validation**: Sanitize all user inputs before processing
3. **Model Verification**: Always verify model integrity using cryptographic hashes
4. **Minimal Permissions**: Request only necessary permissions in extension manifest

### Performance Optimization

1. **Quantization**: Use 4-bit quantized models for optimal memory/performance balance
2. **Caching**: Implement multi-level caching for frequently used prompts
3. **Resource Management**: Monitor and limit memory usage to prevent system impact
4. **Async Operations**: Use non-blocking operations for all I/O operations

### User Experience

1. **Progressive Loading**: Show progress indicators during model loading
2. **Graceful Degradation**: Provide fallback options when local inference fails
3. **Configurability**: Allow users to adjust model parameters and behavior
4. **Transparency**: Provide clear information about model usage and performance

### Maintenance and Updates

1. **Modular Architecture**: Design components for easy updates and maintenance
2. **Version Control**: Use semantic versioning for both extension and models
3. **Telemetry**: Implement optional telemetry to understand usage patterns
4. **Documentation**: Maintain comprehensive documentation for setup and usage

This MasterPrompt provides a complete, research-backed implementation guide for building VS Code extensions with local LLM inference capabilities. The architecture leverages cutting-edge technologies while maintaining security, performance, and user experience best practices identified through comprehensive industry research.
